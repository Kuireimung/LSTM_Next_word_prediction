{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQIBUjzlAJR+Xx1VS/o203",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kuireimung/LSTM_Next_word_prediction/blob/main/LSTM_Next_word_prediction_day7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets huggingface_hub fsspec"
      ],
      "metadata": {
        "id": "rnqbOnXgY4l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0A7JGH8Ys07"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "91m_6alBYwed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access splits\n",
        "train_data = dataset[\"train\"]\n",
        "valid_data = dataset[\"validation\"]\n",
        "test_data = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "6KBmcm2wY9KF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show sample lines\n",
        "for i in range(5):\n",
        "    print(train_data[i][\"text\"])"
      ],
      "metadata": {
        "id": "JBixw-CmY_Oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Text Cleaning (no tokenization)\n",
        "\n",
        "\n",
        "def clean_text(data_split):\n",
        "  # Remove leading/trailing whitespace and filter out empty lines\n",
        "  cleaned_lines = []\n",
        "  for entry in data_split:\n",
        "      line = entry[\"text\"].strip()\n",
        "      if line:  # skip empty lines\n",
        "          cleaned_lines.append(line)\n",
        "  return cleaned_lines\n",
        "\n",
        "cleaned_lines_train = clean_text(train_data)\n",
        "cleaned_lines_valid = clean_text(valid_data)\n",
        "cleaned_lines_test = clean_text(test_data)\n",
        "# Show first 5 cleaned lines\n",
        "for i in range(5):\n",
        "    print(cleaned_lines_train[i])\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(cleaned_lines_valid[i])\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "    print(cleaned_lines_test[i])"
      ],
      "metadata": {
        "id": "cSOYe2EdZBiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def simple_tokenizer(text):\n",
        "    # Lowercase, split on word boundaries and keep punctuation\n",
        "    return re.findall(r\"\\w+|[^\\w\\s]\", text.lower(), re.UNICODE)\n",
        "\n",
        "# Tokenize cleaned lines\n",
        "tokenized_lines_train = [simple_tokenizer(line) for line in cleaned_lines_train if line.strip()]\n",
        "tokenized_lines_valid = [simple_tokenizer(line) for line in cleaned_lines_valid if line.strip()]\n",
        "tokenized_lines_test = [simple_tokenizer(line) for line in cleaned_lines_test if line.strip()]\n",
        "\n",
        "# Show first 5 tokenized lines\n",
        "for i in range(5):\n",
        "    print(tokenized_lines_train[i])\n",
        "for i in range(5):\n",
        "    print(tokenized_lines_valid[i])\n",
        "for i in range(5):\n",
        "    print(tokenized_lines_test[i])\n",
        "\n"
      ],
      "metadata": {
        "id": "Nrpe708kZDpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "tokenized_lines= tokenized_lines_train + tokenized_lines_valid + tokenized_lines_test\n",
        "\n",
        "print(\"total sample: \",  len(tokenized_lines))\n",
        "\n",
        "# Flatten all tokens into a single list\n",
        "all_tokens = [token for line in tokenized_lines for token in line]\n",
        "\n",
        "# Count token frequencies\n",
        "token_freq = Counter(all_tokens)\n",
        "\n",
        "# Build vocabulary: start indexing from 2 (reserve 0 for <pad>, 1 for <unk>)\n",
        "vocab = {token: idx + 2 for idx, (token, _) in enumerate(token_freq.most_common())}\n",
        "vocab[\"<pad>\"] = 0\n",
        "vocab[\"<unk>\"] = 1\n",
        "\n",
        "# Create inverse vocabulary for decoding later\n",
        "inv_vocab = {idx: token for token, idx in vocab.items()}\n",
        "\n",
        "# Show a few items\n",
        "print(\"Vocabulary size:\", len(vocab))\n",
        "print(\"Sample vocab entries:\", list(vocab.items())[:10])\n"
      ],
      "metadata": {
        "id": "RZY7KMHCZFab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Numericalization\n",
        "\n",
        "# Convert each token in tokenized lines to its corresponding index\n",
        "def numericalization(tokenized_line):\n",
        "  numericalized_lines = []\n",
        "  for line in tokenized_line:\n",
        "      encoded = [vocab.get(token, vocab[\"<unk>\"]) for token in line]\n",
        "      numericalized_lines.append(encoded)\n",
        "  return numericalized_lines\n",
        "\n",
        "\n",
        "numericalized_lines_train = numericalization(tokenized_lines_train)\n",
        "numericalized_lines_valid = numericalization(tokenized_lines_valid)\n",
        "numericalized_lines_test = numericalization(tokenized_lines_test)\n",
        "\n",
        "# Show first 5 numericalized lines\n",
        "for i in range(5):\n",
        "    print(numericalized_lines_train[i])\n",
        "for i in range(5):\n",
        "    print(numericalized_lines_valid[i])\n",
        "for i in range(5):\n",
        "    print(numericalized_lines_test[i])\n"
      ],
      "metadata": {
        "id": "3tHkvgI0ZHlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define context window size (sequence length)\n",
        "sequence_length = 5  # You can change this\n",
        "\n",
        "# Function to convert numericalized lines into (input, target) pairs\n",
        "def create_input_target_pairs(numericalized_lines, sequence_length):\n",
        "    pairs = []\n",
        "    for line in numericalized_lines:\n",
        "        if len(line) > sequence_length:\n",
        "            for i in range(len(line) - sequence_length):\n",
        "                input_seq = line[i:i + sequence_length]\n",
        "                target = line[i + sequence_length]\n",
        "                pairs.append((input_seq, target))\n",
        "    return pairs\n",
        "\n",
        "# Apply to each dataset split\n",
        "train_pairs = create_input_target_pairs(numericalized_lines_train, sequence_length)\n",
        "valid_pairs = create_input_target_pairs(numericalized_lines_valid, sequence_length)\n",
        "test_pairs  = create_input_target_pairs(numericalized_lines_test, sequence_length)\n",
        "\n",
        "# Show first 5 (input, target) pairs from train\n",
        "for i in range(5):\n",
        "    print(\"Input:\", train_pairs[i][0], \"→ Target:\", train_pairs[i][1])\n"
      ],
      "metadata": {
        "id": "pFIcqvFDZJkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Custom Dataset class\n",
        "class NextWordDataset(Dataset):\n",
        "    def __init__(self, pairs):\n",
        "        self.pairs = pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_seq, target = self.pairs[idx]\n",
        "        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = NextWordDataset(train_pairs)\n",
        "valid_dataset = NextWordDataset(valid_pairs)\n",
        "test_dataset  = NextWordDataset(test_pairs)\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 64\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Check one batch\n",
        "for X, y in train_loader:\n",
        "    print(\"Input batch shape:\", X.shape)\n",
        "    print(\"Target batch shape:\", y.shape)\n",
        "    print(\"Example input:\", X[0])\n",
        "    print(\"Example target:\", y[0])\n",
        "    break\n"
      ],
      "metadata": {
        "id": "8AlS-RdFZLWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, sequence_length]\n",
        "        embeds = self.embedding(x)  # [batch_size, sequence_length, embedding_dim]\n",
        "        lstm_out, _ = self.lstm(embeds)  # [batch_size, sequence_length, hidden_dim]\n",
        "        last_output = lstm_out[:, -1, :]  # Take the last output for next-word prediction\n",
        "        logits = self.fc(last_output)  # [batch_size, vocab_size]\n",
        "        return logits\n",
        "\n",
        "# Define model with hyperparameters\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "# Print model summary\n",
        "print(model)\n"
      ],
      "metadata": {
        "id": "6LtFEyaTZNo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Set device (use GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "Le9auN_bZPpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "# Training loop\n",
        "num_epochs = 1\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\")\n",
        "\n",
        "    for inputs, targets in train_loader_tqdm:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_perplexity = math.exp(avg_train_loss)\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        val_loader_tqdm = tqdm(valid_loader, desc=f\"Epoch {epoch+1} [Validation]\")\n",
        "        for inputs, targets in val_loader_tqdm:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            val_loss += loss.item()\n",
        "            val_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_val_loss = val_loss / len(valid_loader)\n",
        "    val_perplexity = math.exp(avg_val_loss)\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Summary:\")\n",
        "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Perplexity: {train_perplexity:.2f}\")\n",
        "    print(f\"Val   Loss: {avg_val_loss:.4f}, Val   Perplexity: {val_perplexity:.2f}\\n\")\n"
      ],
      "metadata": {
        "id": "t6t4TMPbZRsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Step 10: Evaluate the Model on Test Set\n",
        "def evaluate_model(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        test_loader_tqdm = tqdm(dataloader, desc=\"Testing\")\n",
        "        for inputs, targets in test_loader_tqdm:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "            test_loader_tqdm.set_postfix(loss=loss.item())\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    return avg_loss, perplexity\n",
        "\n",
        "# Run evaluation\n",
        "test_loss, test_perplexity = evaluate_model(model, test_loader, criterion)\n",
        "\n",
        "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
        "print(f\"Test Perplexity: {test_perplexity:.2f}\")\n"
      ],
      "metadata": {
        "id": "PU8uN0kjZT8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Create output directory if not exists\n",
        "os.makedirs(\"model_output\", exist_ok=True)\n",
        "\n",
        "# Save model state\n",
        "model_path = \"model_output/lstm_nextword_model.pt\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "# Save vocab as JSON\n",
        "vocab_path = \"model_output/vocab.json\"\n",
        "with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(vocab, f)\n",
        "\n",
        "# Save inverse vocab as well (optional, for decoding)\n",
        "inv_vocab_path = \"model_output/inv_vocab.json\"\n",
        "with open(inv_vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(inv_vocab, f)\n",
        "\n",
        "print(f\"✅ Model saved to: {model_path}\")\n",
        "print(f\"✅ Vocab saved to: {vocab_path}\")\n"
      ],
      "metadata": {
        "id": "Xd_xhJXiZWC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import json\n",
        "\n",
        "# --- Load saved vocab and model ---\n",
        "with open(\"model_output/vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    vocab = json.load(f)\n",
        "with open(\"model_output/inv_vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    inv_vocab = {int(k): v for k, v in json.load(f).items()}\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "sequence_length = 5\n",
        "\n",
        "# Define the same LSTM model architecture\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        last_output = lstm_out[:, -1, :]\n",
        "        logits = self.fc(last_output)\n",
        "        return logits\n",
        "\n",
        "# Load model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim)\n",
        "model.load_state_dict(torch.load(\"model_output/lstm_nextword_model.pt\", map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# --- Text generation ---\n",
        "def generate_next_words(prompt, num_words):\n",
        "    tokens = prompt.strip().lower().split()\n",
        "    indices = [vocab.get(tok, vocab[\"<unk>\"]) for tok in tokens]\n",
        "\n",
        "    for _ in range(num_words):\n",
        "        # Pad/truncate to fixed sequence length\n",
        "        input_seq = indices[-sequence_length:]\n",
        "        if len(input_seq) < sequence_length:\n",
        "            input_seq = [vocab[\"<pad>\"]] * (sequence_length - len(input_seq)) + input_seq\n",
        "        input_tensor = torch.tensor([input_seq], dtype=torch.long).to(device)\n",
        "\n",
        "        # Predict next word\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_tensor)\n",
        "            predicted_index = torch.argmax(logits, dim=1).item()\n",
        "            predicted_token = inv_vocab.get(predicted_index, \"<unk>\")\n",
        "\n",
        "        indices.append(predicted_index)\n",
        "\n",
        "    # Convert all indices back to tokens\n",
        "    output_tokens = [inv_vocab.get(idx, \"<unk>\") for idx in indices]\n",
        "    return \" \".join(output_tokens)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FxW48pjTZYds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Example ---\n",
        "user_prompt = \"India is a land of\"\n",
        "N = 10\n",
        "output = generate_next_words(user_prompt, N)\n",
        "print(\"\\nGenerated text:\")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "uBw_OeYqZaTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(device)"
      ],
      "metadata": {
        "id": "HNYa_jLyZckj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}