{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOm7vEV8cnr2suwUz/cOAoY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Implementation Plan: LSTM-based Next Word Prediction (Text-to-Text Generation)**\n","\n","---\n","\n","### ðŸ”¹ **Step 1: Load the Dataset**\n","\n","**What we do**:\n","Load the **WikiText-2** dataset from Hugging Face ([`wikitext-2-raw-v1`](https://www.salesforce.com/blog/the-wikitext-long-term-dependency-language-modeling-dataset/)) or TorchText.\n","\n","**Dataset Description**:\n","\n","* A cleaned, curated subset of **Wikipedia articles**.\n","* Maintains natural sentence structure and punctuation.\n","* Specifically designed for **language modeling** and **next-token prediction** tasks.\n","\n","**Statistics**:\n","\n","| Split | # Articles | # Tokens    | Vocab Size |\n","| ----- | ---------- | ----------- | ---------- |\n","| Train | \\~600      | \\~2 million | \\~33,000   |\n","| Valid | \\~60       | \\~217,000   | \\~12,000   |\n","| Test  | \\~60       | \\~245,000   | \\~12,000   |\n","\n","**Why this step**:\n","Gives us real-world, high-quality English text suitable to train a language model (next word predictor).\n","\n","---\n","\n","### ðŸ”¹ **Step 2: Text Cleaning (Optional)**\n","\n","**What we do**:\n","\n","* Remove empty lines, special characters, or unnecessary headers.\n","* Normalize case if needed (e.g., lowercase all text).\n","\n","**Examples**:\n","\n","| Raw Line                      | Cleaned Output             |\n","| ----------------------------- | -------------------------- |\n","| `= Valkyria Chronicles III =` | *(removed)*                |\n","| `The story takes place...`    | `The story takes place...` |\n","| ` `                           | *(removed blank line)*     |\n","\n","\n","**Why**:\n","\n","* Prepares clean, meaningful input for the tokenizer.\n","* Ensures the model isnâ€™t biased by formatting artifacts.\n","\n","---\n","\n","### ðŸ”¹ **Step 3: Tokenization**\n","\n","**What we do**:\n","\n","* Break down each line of text into individual **tokens** (usually words or subwords).\n","* Use simple whitespace or `basic_english` (lowercasing + punctuation splitting) tokenizer.\n","\n","**Examples**:\n","\n","| Input Sentence            | Tokenized Output                                 |\n","| ------------------------- | ------------------------------------------------ |\n","| `The cat sat on the mat.` | `['the', 'cat', 'sat', 'on', 'the', 'mat', '.']` |\n","| `He didn't know.`         | `['he', 'didn', \"'\", 't', 'know', '.']`          |\n","\n","**Why**:\n","\n","* Language models work on tokensâ€”not raw strings.\n","* Tokenization defines the unit of prediction (e.g., word vs subword).\n","\n","---\n","\n","### ðŸ”¹ **Step 4: Vocabulary Creation**\n","\n","**What we do**:\n","\n","* Build a **vocabulary dictionary**: `token â†’ index` and `index â†’ token`.\n","* Assign special tokens like `<pad>` and `<unk>`.\n","\n","**Examples**:\n","\n","| Token   | Index |\n","| ------- | ----- |\n","| `<pad>` | 0     |\n","| `<unk>` | 1     |\n","| `the`   | 2     |\n","| `cat`   | 3     |\n","| `sat`   | 4     |\n","| `.`     | 5     |\n","\n","\n","**Why**:\n","\n","* Converts words into numerical IDs (**integers**) that can be processed by neural networks.\n","* Allows consistent mapping from token strings to embeddings.\n","\n","---\n","\n","### ðŸ”¹ **Step 5: Numericalization**\n","\n","**What we do**:\n","\n","* Convert the tokenized corpus into a long list of integers using the vocabulary.\n","\n","**Example**:\n","\n","| Tokens                  | Encoded     |\n","| ----------------------- | ----------- |\n","| `['the', 'cat', 'sat']` | `[2, 3, 4]` |\n","\n","\n","**Why**:\n","\n","* Neural networks only understand numbers.\n","* Numericalization allows us to feed sequences into embedding layers.\n","\n","---\n","\n","### ðŸ”¹ **Step 6: Create (Input, Target) Training Pairs**\n","\n","**What we do**:\n","\n","* For a given sequence length `n`:\n","\n","  * Input = first `n` tokens (e.g., `\"the cat sat on\"`)\n","  * Target = next token (`\"the cat sat on â†’ mat\"`)\n","  \n","  \n","  | Input (X)               | Target (y) |\n","  | ----------------------- | ---------- |\n","  | `['the', 'cat', 'sat']` | `'on'`     |\n","  | `['cat', 'sat', 'on']`  | `'the'`    |\n","\n","  Then numericalize:\n","\n","  * `X = [2, 3, 4]`\n","  * `y = 5` (index of `'on'`)\n","\n","\n","**Why**:\n","\n","* This forms the supervised data to train next-word prediction.\n","* The model learns to predict the next word given a context window.\n","\n","---\n","\n","### ðŸ”¹ **Step 7: Dataset and DataLoader (Batching)**\n","\n","**What we do**:\n","\n","* Wrap input-output pairs in a custom `Dataset` class.\n","* Use PyTorchâ€™s `DataLoader` to batch and shuffle the data.\n","\n","**Why**:\n","\n","* Batching speeds up training.\n","* DataLoader automates shuffling, batching, and parallel loading.\n","\n","---\n","\n","### ðŸ”¹ **Step 8: Define the LSTM Model**\n","\n","**What we do**:\n","\n","* Build a simple neural network:\n","\n","  * Embedding layer â†’ LSTM â†’ Fully Connected layer â†’ Vocabulary size logits\n","\n","**Why**:\n","\n","* LSTM captures temporal sequence information.\n","* Embedding layer maps word IDs to dense vectors.\n","* Output layer predicts the next word by scoring each token in the vocab.\n","\n","---\n","\n","### ðŸ”¹ **Step 9: Train the Model**\n","\n","**What we do**:\n","\n","* Loop over batches, compute loss (`CrossEntropy`), backpropagate, and update weights using an optimizer (e.g., Adam).\n","\n","**Why**:\n","\n","* The model minimizes the difference between predicted and actual next tokens.\n","* Trains the model to assign high probability to correct next word.\n","\n","---\n","\n","### ðŸ”¹ **Step 10: Evaluate the Model (Optional)**\n","\n","**What we do**:\n","\n","* Measure **perplexity** on validation data.\n","* Try generating predictions for a few sample input sequences.\n","\n","**Why**:\n","\n","* Perplexity is a standard metric for next-word prediction.\n","* Sample outputs show how well the model generalizes to unseen inputs.\n","\n","---\n","\n","### ðŸ”¹ **Step 11: Save the Model (Optional)**\n","\n","**What we do**:\n","\n","* Save model weights and vocabulary for future use.\n","\n","**Why**:\n","\n","* Enables reuse for inference, fine-tuning, or switching to a Transformer later.\n","\n","---"],"metadata":{"id":"epUzrhn61NTv"}},{"cell_type":"markdown","source":["# **Step 0: Install python packages**"],"metadata":{"id":"95L9zsyN5ZWx"}},{"cell_type":"code","source":["!pip install -U datasets huggingface_hub fsspec"],"metadata":{"id":"nd6Y_TYk0okC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step 1: Load the Dataset**"],"metadata":{"id":"YoC6p2pH6mlz"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vOPOsOZ_uSIt"},"outputs":[],"source":["from datasets import load_dataset"]},{"cell_type":"code","source":["dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n","print(dataset)\n"],"metadata":{"id":"EU1oPajIuqDf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Access splits\n","train_data = dataset[\"train\"]\n","valid_data = dataset[\"validation\"]\n","test_data = dataset[\"test\"]"],"metadata":{"id":"bUrB1r6gwYxF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Show sample lines\n","for i in range(5):\n","    print(train_data[i][\"text\"])"],"metadata":{"id":"JhjUKSGgwi2b"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step 2: Text Cleaning**\n","1. Remove leading/trailing whitespace and filter out empty lines\n","2. Do not remove Prediction\n","  \n","  a. The model needs to learn how punctuation affects meaning and sentence boundaries.\n","\n","  b. Punctuation like ```., ,, ?, !``` is part of the language structure and influences what the next word could be."],"metadata":{"id":"y24wdans7YmU"}},{"cell_type":"code","source":["# Step 2: Text Cleaning (no tokenization)\n","\n","\n","def clean_text(data_split):\n","  # Remove leading/trailing whitespace and filter out empty lines\n","  cleaned_lines = []\n","  for entry in data_split:\n","      line = entry[\"text\"].strip()\n","      if line:  # skip empty lines\n","          cleaned_lines.append(line)\n","  return cleaned_lines\n","\n","cleaned_lines_train = clean_text(train_data)\n","cleaned_lines_valid = clean_text(valid_data)\n","cleaned_lines_test = clean_text(test_data)\n","# Show first 5 cleaned lines\n","for i in range(5):\n","    print(cleaned_lines_train[i])\n","\n","\n","for i in range(5):\n","    print(cleaned_lines_valid[i])\n","\n","\n","for i in range(5):\n","    print(cleaned_lines_test[i])"],"metadata":{"id":"NLHCvOJKvljC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step 3: Tokenization**"],"metadata":{"id":"lKSi5vvt9czU"}},{"cell_type":"code","source":["import re\n","\n","def simple_tokenizer(text):\n","    # Lowercase, split on word boundaries and keep punctuation\n","    return re.findall(r\"\\w+|[^\\w\\s]\", text.lower(), re.UNICODE)\n","\n","# Tokenize cleaned lines\n","tokenized_lines_train = [simple_tokenizer(line) for line in cleaned_lines_train if line.strip()]\n","tokenized_lines_valid = [simple_tokenizer(line) for line in cleaned_lines_valid if line.strip()]\n","tokenized_lines_test = [simple_tokenizer(line) for line in cleaned_lines_test if line.strip()]\n","\n","# Show first 5 tokenized lines\n","for i in range(5):\n","    print(tokenized_lines_train[i])\n","for i in range(5):\n","    print(tokenized_lines_valid[i])\n","for i in range(5):\n","    print(tokenized_lines_test[i])\n","\n"],"metadata":{"id":"opcUNR4FwQ-h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step 4: Vocabulary Creation**"],"metadata":{"id":"Z-0saXF3_R7H"}},{"cell_type":"code","source":["from collections import Counter\n","\n","tokenized_lines= tokenized_lines_train + tokenized_lines_valid + tokenized_lines_test\n","\n","print(\"total sample: \",  len(tokenized_lines))\n","\n","# Flatten all tokens into a single list\n","all_tokens = [token for line in tokenized_lines for token in line]\n","\n","# Count token frequencies\n","token_freq = Counter(all_tokens)\n","\n","# Build vocabulary: start indexing from 2 (reserve 0 for <pad>, 1 for <unk>)\n","vocab = {token: idx + 2 for idx, (token, _) in enumerate(token_freq.most_common())}\n","vocab[\"<pad>\"] = 0\n","vocab[\"<unk>\"] = 1\n","\n","# Create inverse vocabulary for decoding later\n","inv_vocab = {idx: token for token, idx in vocab.items()}\n","\n","# Show a few items\n","print(\"Vocabulary size:\", len(vocab))\n","print(\"Sample vocab entries:\", list(vocab.items())[:10])\n"],"metadata":{"id":"cFQadGAP_HOj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step 5: Numericalization**"],"metadata":{"id":"vKsD-bh1_-RT"}},{"cell_type":"code","source":["# Step 5: Numericalization\n","\n","# Convert each token in tokenized lines to its corresponding index\n","def numericalization(tokenized_line):\n","  numericalized_lines = []\n","  for line in tokenized_line:\n","      encoded = [vocab.get(token, vocab[\"<unk>\"]) for token in line]\n","      numericalized_lines.append(encoded)\n","  return numericalized_lines\n","\n","\n","numericalized_lines_train = numericalization(tokenized_lines_train)\n","numericalized_lines_valid = numericalization(tokenized_lines_valid)\n","numericalized_lines_test = numericalization(tokenized_lines_test)\n","\n","# Show first 5 numericalized lines\n","for i in range(5):\n","    print(numericalized_lines_train[i])\n","for i in range(5):\n","    print(numericalized_lines_valid[i])\n","for i in range(5):\n","    print(numericalized_lines_test[i])\n"],"metadata":{"id":"Ttv7rtzr_gU4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step 6: Create (Input, Target) Training Pairs.**"],"metadata":{"id":"g0QmbBCvNpoA"}},{"cell_type":"code","source":["# Define context window size (sequence length)\n","sequence_length = 5  # You can change this\n","\n","# Function to convert numericalized lines into (input, target) pairs\n","def create_input_target_pairs(numericalized_lines, sequence_length):\n","    pairs = []\n","    for line in numericalized_lines:\n","        if len(line) > sequence_length:\n","            for i in range(len(line) - sequence_length):\n","                input_seq = line[i:i + sequence_length]\n","                target = line[i + sequence_length]\n","                pairs.append((input_seq, target))\n","    return pairs\n","\n","# Apply to each dataset split\n","train_pairs = create_input_target_pairs(numericalized_lines_train, sequence_length)\n","valid_pairs = create_input_target_pairs(numericalized_lines_valid, sequence_length)\n","test_pairs  = create_input_target_pairs(numericalized_lines_test, sequence_length)\n","\n","# Show first 5 (input, target) pairs from train\n","for i in range(5):\n","    print(\"Input:\", train_pairs[i][0], \"â†’ Target:\", train_pairs[i][1])\n"],"metadata":{"id":"3nCqreDWNCJz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step 7: Wrap into Dataset and DataLoader**"],"metadata":{"id":"aXVC7f9lOf6E"}},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","# Custom Dataset class\n","class NextWordDataset(Dataset):\n","    def __init__(self, pairs):\n","        self.pairs = pairs\n","\n","    def __len__(self):\n","        return len(self.pairs)\n","\n","    def __getitem__(self, idx):\n","        input_seq, target = self.pairs[idx]\n","        return torch.tensor(input_seq, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n","\n","# Create datasets\n","train_dataset = NextWordDataset(train_pairs)\n","valid_dataset = NextWordDataset(valid_pairs)\n","test_dataset  = NextWordDataset(test_pairs)\n","\n","# DataLoaders\n","batch_size = 64\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n","test_loader  = DataLoader(test_dataset, batch_size=batch_size)\n","\n","# Check one batch\n","for X, y in train_loader:\n","    print(\"Input batch shape:\", X.shape)\n","    print(\"Target batch shape:\", y.shape)\n","    print(\"Example input:\", X[0])\n","    print(\"Example target:\", y[0])\n","    break\n"],"metadata":{"id":"z1rk8pDAOQZX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step 8: Define the LSTM Model**"],"metadata":{"id":"0gVJwrNEO4Yr"}},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class LSTMModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n","        super(LSTMModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, x):\n","        # x: [batch_size, sequence_length]\n","        embeds = self.embedding(x)  # [batch_size, sequence_length, embedding_dim]\n","        lstm_out, _ = self.lstm(embeds)  # [batch_size, sequence_length, hidden_dim]\n","        last_output = lstm_out[:, -1, :]  # Take the last output for next-word prediction\n","        logits = self.fc(last_output)  # [batch_size, vocab_size]\n","        return logits\n","\n","# Define model with hyperparameters\n","embedding_dim = 128\n","hidden_dim = 256\n","vocab_size = len(vocab)\n","\n","model = LSTMModel(vocab_size, embedding_dim, hidden_dim)\n","\n","# Print model summary\n","print(model)\n"],"metadata":{"id":"tWt8TqzaOjVy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step 9: Train the Model.**"],"metadata":{"id":"imuQBQFuPCxq"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","# Set device (use GPU if available)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","# Loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)"],"metadata":{"id":"qGKRZ67I055g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import math\n","from tqdm import tqdm\n","# Training loop\n","num_epochs = 1\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Training]\")\n","\n","    for inputs, targets in train_loader_tqdm:\n","        inputs, targets = inputs.to(device), targets.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","        train_loader_tqdm.set_postfix(loss=loss.item())\n","\n","    avg_train_loss = total_loss / len(train_loader)\n","    train_perplexity = math.exp(avg_train_loss)\n","\n","    # Validation loop\n","    model.eval()\n","    val_loss = 0\n","    with torch.no_grad():\n","        val_loader_tqdm = tqdm(valid_loader, desc=f\"Epoch {epoch+1} [Validation]\")\n","        for inputs, targets in val_loader_tqdm:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            val_loss += loss.item()\n","            val_loader_tqdm.set_postfix(loss=loss.item())\n","\n","    avg_val_loss = val_loss / len(valid_loader)\n","    val_perplexity = math.exp(avg_val_loss)\n","\n","    print(f\"\\nEpoch {epoch+1} Summary:\")\n","    print(f\"Train Loss: {avg_train_loss:.4f}, Train Perplexity: {train_perplexity:.2f}\")\n","    print(f\"Val   Loss: {avg_val_loss:.4f}, Val   Perplexity: {val_perplexity:.2f}\\n\")\n"],"metadata":{"id":"P87pohxkO6wS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Step 10: Evaluate the Model**"],"metadata":{"id":"vrNV1Vch1myz"}},{"cell_type":"code","source":["import math\n","from tqdm import tqdm\n","\n","# Step 10: Evaluate the Model on Test Set\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        test_loader_tqdm = tqdm(dataloader, desc=\"Testing\")\n","        for inputs, targets in test_loader_tqdm:\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, targets)\n","            total_loss += loss.item()\n","            test_loader_tqdm.set_postfix(loss=loss.item())\n","\n","    avg_loss = total_loss / len(dataloader)\n","    perplexity = math.exp(avg_loss)\n","    return avg_loss, perplexity\n","\n","# Run evaluation\n","test_loss, test_perplexity = evaluate_model(model, test_loader, criterion)\n","\n","print(f\"\\nTest Loss: {test_loss:.4f}\")\n","print(f\"Test Perplexity: {test_perplexity:.2f}\")\n"],"metadata":{"id":"eRAc1HMXPHnW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Step 11: Save the Model and Vocab**"],"metadata":{"id":"43rx9D8B10M0"}},{"cell_type":"code","source":["import torch\n","import json\n","import os\n","\n","# Create output directory if not exists\n","os.makedirs(\"model_output\", exist_ok=True)\n","\n","# Save model state\n","model_path = \"model_output/lstm_nextword_model.pt\"\n","torch.save(model.state_dict(), model_path)\n","\n","# Save vocab as JSON\n","vocab_path = \"model_output/vocab.json\"\n","with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(vocab, f)\n","\n","# Save inverse vocab as well (optional, for decoding)\n","inv_vocab_path = \"model_output/inv_vocab.json\"\n","with open(inv_vocab_path, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(inv_vocab, f)\n","\n","print(f\"âœ… Model saved to: {model_path}\")\n","print(f\"âœ… Vocab saved to: {vocab_path}\")\n"],"metadata":{"id":"BtW5UysJZqQP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Inference**"],"metadata":{"id":"0-63sFGN5Xa-"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import json\n","\n","# --- Load saved vocab and model ---\n","with open(\"model_output/vocab.json\", \"r\", encoding=\"utf-8\") as f:\n","    vocab = json.load(f)\n","with open(\"model_output/inv_vocab.json\", \"r\", encoding=\"utf-8\") as f:\n","    inv_vocab = {int(k): v for k, v in json.load(f).items()}\n","\n","vocab_size = len(vocab)\n","embedding_dim = 128\n","hidden_dim = 256\n","sequence_length = 5\n","\n","# Define the same LSTM model architecture\n","class LSTMModel(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n","        super(LSTMModel, self).__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n","        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, vocab_size)\n","\n","    def forward(self, x):\n","        embeds = self.embedding(x)\n","        lstm_out, _ = self.lstm(embeds)\n","        last_output = lstm_out[:, -1, :]\n","        logits = self.fc(last_output)\n","        return logits\n","\n","# Load model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = LSTMModel(vocab_size, embedding_dim, hidden_dim)\n","model.load_state_dict(torch.load(\"model_output/lstm_nextword_model.pt\", map_location=device))\n","model.to(device)\n","model.eval()\n","\n","# --- Text generation ---\n","def generate_next_words(prompt, num_words):\n","    tokens = prompt.strip().lower().split()\n","    indices = [vocab.get(tok, vocab[\"<unk>\"]) for tok in tokens]\n","\n","    for _ in range(num_words):\n","        # Pad/truncate to fixed sequence length\n","        input_seq = indices[-sequence_length:]\n","        if len(input_seq) < sequence_length:\n","            input_seq = [vocab[\"<pad>\"]] * (sequence_length - len(input_seq)) + input_seq\n","        input_tensor = torch.tensor([input_seq], dtype=torch.long).to(device)\n","\n","        # Predict next word\n","        with torch.no_grad():\n","            logits = model(input_tensor)\n","            predicted_index = torch.argmax(logits, dim=1).item()\n","            predicted_token = inv_vocab.get(predicted_index, \"<unk>\")\n","\n","        indices.append(predicted_index)\n","\n","    # Convert all indices back to tokens\n","    output_tokens = [inv_vocab.get(idx, \"<unk>\") for idx in indices]\n","    return \" \".join(output_tokens)\n","\n","\n"],"metadata":{"id":"sZ0ja18U2LN7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --- Example ---\n","user_prompt = \"India is a land of\"\n","N = 10\n","output = generate_next_words(user_prompt, N)\n","print(\"\\nGenerated text:\")\n","print(output)"],"metadata":{"id":"EN0L5p1Y2LKl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Exercise 1**\n","Write a code for inference that generates complete sentences."],"metadata":{"id":"830c8Xpi5oZK"}},{"cell_type":"code","source":[],"metadata":{"id":"dEzUY4j8ZtEu"},"execution_count":null,"outputs":[]}]}